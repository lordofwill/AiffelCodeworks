{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comic-lyric",
   "metadata": {},
   "source": [
    "## package import\n",
    "---\n",
    "한글 명칭이 명확하지 않은 용어를 굳이 한글화하는 것보다는 영어 명칭을 적는 것이 더 깔끔하다고 생각되어\n",
    "영어명칭으로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "angry-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "small-aquarium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def init_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\\n\\n\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e, \"\\n\\n\")\n",
    "        \n",
    "init_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-offer",
   "metadata": {},
   "source": [
    "랜덤시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unknown-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1111\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-slope",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nasty-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_path = os.path.join(os.getenv('HOME'), '/aiffel/aiffel/bert_pretrain')\n",
    "data_dir_path = os.path.join(workspace_path, 'data')\n",
    "model_dir_path = os.path.join(workspace_path, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "speaking-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file_path = os.path.join(data_dir_path, 'kowiki.txt')\n",
    "prefix = os.path.join(model_dir_path, 'ko_8000')\n",
    "vocab_size = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-command",
   "metadata": {},
   "source": [
    "## Setting Tokenizer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "opening-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file_path} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "higher-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{prefix}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "multiple-victorian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다', '었다', '▁지', '▁수', '▁19', '▁가', '▁시', '▁20', '▁기', '▁전', '▁아', '▁하', '▁있다', '▁다', '▁제', '했다', '하였', '▁일', '▁한', '▁중', '▁정', '▁주', '하는', '▁것', '▁자', '▁공', '▁인', '되었다', '▁경', '▁위', '▁유', '▁보', '하고', '▁3', '▁등', '▁부', '하였다', '▁조', '하여', '▁미', '▁동', '▁선', '▁나', '으며', '▁모', '▁연', '▁영', '▁의', '▁오', '▁마', '에는', '▁발', '▁소', '한다', '▁고', '▁개', '▁201', '▁구', '▁세', '▁도', '▁상', '▁비', '▁스', '▁국', '▁서', '▁후', '▁여', '▁200', '▁때', '▁4', '▁성', '▁해', '▁관', '▁있는', '▁신', '▁프', '▁대한', '부터', '▁5', '▁방', '▁또', '지만', '▁(', '▁역', '되어', '▁않', '▁만', '▁\"', '▁장', '▁바', '까지']\n"
     ]
    }
   ],
   "source": [
    "# 특수 token 7개를 제외한 나머지 tokens 들\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "print(vocab_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "circular-pittsburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-stock",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "homeless-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "foster-boring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "radical-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '仇', '雲', '苦', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁비해', '튼', '▁M', '[MASK]', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '[MASK]', '[MASK]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '[MASK]', '[MASK]', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [18, 19, 20, 38, 39, 40, 41, 64, 65, 66, 74, 75, 91, 92]\n",
      "mask_label : ['▁손', '님', '이', '▁받아', '보', '는', '▁십', '▁컬', '컬', '한', '▁적', '셔', '▁먹', '고']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-finder",
   "metadata": {},
   "source": [
    "nsp Pair forming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "norman-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "removed-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "emerging-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "middle-minute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "accepting-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "vocational-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '적', '추', '적', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '[MASK]', '[MASK]', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '[MASK]', '[MASK]', '▁흘', '러', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 6, 7, 25, 26, 40, 59, 60], 'mask_label': ['▁비', '가', '▁내', '리는', '▁삼', '십', '▁십', '▁눈', '물이']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '▁몇', '[MASK]', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '[MASK]', '[MASK]', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 15, 26, 27, 28, 52, 53], 'mask_label': ['▁컬', '컬', '한', '▁달', '▁그', '토', '록', '▁문', '득']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁나', '를', '년까지', '꼽', '서로', '녓', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져', '[SEP]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 4, 5, 6, 7, 56, 57, 58, 59], 'mask_label': ['와', '▁원', '망', '하', '듯', '▁일', '찍', '이라', '도']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]', '▁난', '▁맨', '날', '▁이렇게', '[MASK]', '▁수', '▁있', '으면', '[MASK]', '[MASK]', '▁좋', '을', '까', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [15, 19, 20], 'mask_label': ['▁살', '▁얼마', '나']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-anthropology",
   "metadata": {},
   "source": [
    "데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "domestic-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sustainable-prediction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29888289c2141b88fac2278ac2df90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.path.join(data_dir_path, 'bert_pre_train_8000.json')\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file_path, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "internal-faculty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918189"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sealed-hundred",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "tutorial-trainer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495b9b09658d4dd7bf6b19613355582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁지', '미', '▁카', '터', '[SEP]', '[MASK]', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '[MASK]', '[MASK]', '▁1', '일', '[MASK]', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁지', '미', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '[MASK]', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '[MASK]', '[MASK]', '▁\"', '땅', '콩', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 20, 21, 24, 40, 41, 42, 43, 44, 65, 66, 67, 68, 69, 70, 112, 122, 123], 'mask_label': ['▁제임스', '▁10', '월', '▁~', '▁1981', '년', ')', '이다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁등을', '▁별', '명이']}\n",
      "enc_token: [5, 18, 3686, 207, 3714, 4, 6, 1042, 103, 3610, 3686, 3718, 207, 3714, 37, 3418, 416, 810, 3666, 3625, 6, 6, 7, 3629, 6, 241, 3602, 1114, 3724, 788, 243, 49, 3632, 796, 663, 1647, 3682, 3682, 3625, 203, 6, 6, 6, 6, 6, 18, 3686, 207, 3714, 3602, 1755, 3630, 3646, 630, 3714, 3565, 3835, 429, 3740, 3628, 3626, 1369, 10, 1605, 3599, 6, 6, 6, 6, 6, 6, 1135, 52, 3599, 13, 81, 87, 1501, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 6, 21, 5007, 399, 1927, 3607, 813, 17, 3599, 307, 6, 6, 103, 4313, 4290, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0 3324    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  131 3662    0    0  203    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 3008 3625\n",
      " 3616   16 3599    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 1755 3630   41 3644  830\n",
      " 3624    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  593    0    0    0    0    0    0    0    0    0  587  931    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '아', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '정한', '선', '▁주', '▁지', '사', '▁선거', '에', '[MASK]', '[MASK]', '[MASK]', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', '▁상', '원의', '원을', '[MASK]', '[MASK]', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '▁사는', '▁흑', '인', '[MASK]', '[MASK]', '[MASK]', '▁내', '세', '웠다', '.', '[SEP]', '▁1976', '년', '▁대통령', '▁선거', '에', '[MASK]', '[MASK]', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '▁정책', '으로', '▁내', '세', '워', ',', '▁포', '드를', '▁누', '르고', '[MASK]', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [29, 30, 36, 37, 38, 57, 58, 59, 60, 61, 91, 92, 93, 104, 105, 124, 125, 126], 'mask_label': ['▁조지', '아', '▁낙', '선', '하지만', '▁상', '원의', '원을', '▁두', '번', '▁등', '용', '법을', '▁민주', '당', '▁당선', '되었다', '.']}\n",
      "enc_token: [5, 3630, 37, 76, 3667, 2378, 822, 10, 1567, 3668, 3294, 13, 822, 3608, 2386, 2163, 3596, 3671, 969, 213, 3929, 173, 607, 2387, 317, 3604, 386, 3673, 3625, 1291, 3668, 37, 18, 3620, 822, 3600, 6, 6, 6, 1921, 3625, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 663, 3597, 450, 3614, 25, 1755, 3630, 3646, 76, 955, 928, 6, 6, 61, 3773, 530, 3604, 3372, 523, 3409, 673, 1755, 3630, 18, 982, 2711, 31, 3599, 1755, 3630, 37, 3610, 982, 18, 3754, 151, 3604, 243, 3600, 3554, 1733, 3628, 6, 6, 6, 114, 3692, 1853, 3599, 4, 3306, 3625, 663, 822, 3600, 6, 6, 958, 3603, 117, 3674, 54, 75, 4089, 238, 1421, 9, 114, 3692, 3964, 3604, 119, 1486, 807, 2056, 6, 6, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0 1755 3630    0    0    0    0    0 1567 3668 1447    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   76  955  928  157 3821    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0   50 3717 2046    0    0    0    0\n",
      "    0    0    0    0    0    0 1114 3724    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 2387   43\n",
      " 3599    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '[MASK]', '[MASK]', '[MASK]', '▁전략', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '[MASK]', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '▁지', '키', '기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.', '[SEP]', '▁카', '터', '▁대통령', '은', '▁에너', '지', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '▁반', '대로', '▁무', '산', '되었다', '.', '▁카', '터', '는', '▁이집', '트', '와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '[MASK]', '[MASK]', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '[MASK]', '[MASK]', '▁평', '화를', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [14, 15, 16, 34, 60, 61, 62, 63, 107, 108, 117, 118, 121, 122, 123, 124, 125, 126], 'mask_label': ['▁제', '2', '차', '▁대한민국', '▁내', '세', '웠다', '.', '▁대통령', '과', '▁중', '동', '▁위한', '▁캠', '프', '데', '이', '비']}\n",
      "enc_token: [5, 230, 3643, 2714, 2793, 3676, 3827, 9, 1435, 2521, 3599, 276, 1302, 3644, 6, 6, 6, 2835, 107, 3614, 1956, 617, 1824, 53, 3628, 31, 3599, 207, 3714, 3602, 1921, 596, 1840, 316, 6, 50, 42, 3830, 81, 3713, 137, 968, 247, 42, 917, 18, 3793, 3614, 231, 3375, 530, 3604, 2659, 165, 785, 874, 75, 4089, 3642, 1233, 114, 3692, 1853, 3599, 4, 207, 3714, 663, 3613, 1778, 3610, 570, 3607, 2270, 3653, 1003, 41, 3683, 1547, 141, 448, 107, 3726, 43, 3599, 207, 3714, 3602, 2703, 3677, 3665, 3426, 3607, 3358, 54, 3604, 2432, 3721, 965, 3694, 3552, 172, 3665, 3699, 15, 3598, 3677, 6, 6, 334, 3637, 5887, 271, 4099, 1011, 3644, 280, 6, 6, 232, 934, 6, 6, 6, 6, 6, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   30 3619 3751    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  410    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  114 3692 1853 3599    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  663 3644    0    0    0\n",
      "    0    0    0    0    0   35 3658    0    0  521 2432 3721 3736 3597\n",
      " 3694    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '[MASK]', '[MASK]', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '[MASK]', '[MASK]', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '[MASK]', '[MASK]', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '▁1980', '년', '[MASK]', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.', '[SEP]', '▁지', '미', '▁카', '터', '는', '▁대한민국', '과의', '▁관계', '에서도', '드를', '▁영향을', '▁미', '쳤', '던', '▁대통령', '▁중', '▁하나', '다', '.', '▁인', '권', '▁문제', '와', '▁주', '한', '미', '군', '▁철', '수', '▁문제', '로', '▁한때', '▁한', '미', '▁관계', '가', '▁불', '편', '하기도', '▁했다', '.', '▁1978', '년', '▁대한민국', '에', '▁대한', '[MASK]', '[MASK]', '▁위협', '에', '[MASK]', '[MASK]', '[MASK]', '▁한', '미', '연합', '사를', '▁창설', '하면서', ',', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 10, 11, 22, 23, 33, 34, 49, 74, 111, 112, 113, 114, 115, 116, 117, 125, 126], 'mask_label': ['질', '▁선거', '에서', '▁', '져', '▁말', '기에', '▁하계', '▁중요한', '▁북한', '의', '▁위협', '에', '▁대', '비', '해', '▁1982', '년까지']}\n",
      "enc_token: [5, 6, 73, 3771, 1579, 3624, 1827, 1640, 3625, 663, 6, 6, 41, 3683, 1547, 194, 4044, 3681, 1169, 3803, 958, 113, 6, 6, 875, 174, 2087, 1579, 31, 3599, 276, 273, 3614, 6, 6, 870, 3713, 1302, 3601, 26, 2986, 3733, 1323, 3232, 636, 9, 751, 1640, 3625, 6, 779, 3600, 141, 3670, 3643, 3608, 247, 3052, 4805, 3607, 114, 3692, 1853, 3599, 4, 18, 3686, 207, 3714, 3602, 410, 786, 704, 643, 1486, 1063, 55, 4219, 3781, 663, 35, 324, 3598, 3599, 42, 3830, 550, 3665, 37, 3612, 3686, 3722, 380, 3636, 550, 3603, 3590, 34, 3686, 704, 3608, 128, 3877, 863, 345, 3599, 3331, 3625, 410, 3600, 92, 6, 6, 3038, 3600, 6, 6, 6, 34, 3686, 2569, 451, 3574, 421, 3604, 6, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0 3892    0    0    0    0    0    0    0    0  822   10    0    0\n",
      "    0    0    0    0    0    0    0    0 3596 3944    0    0    0    0\n",
      "    0    0    0    0    0  150  329    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 2219    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0 1165    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 1876\n",
      " 3601 3038 3600   14 3694 3645    0    0    0    0    0    0    0 2760\n",
      "  673    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '▁위해', '[MASK]', '▁3', '세', '계의', '▁선거', '▁감', '시', '[MASK]', '▁및', '[MASK]', '[MASK]', '▁벌', '레', '에', '▁의한', '▁드', '라', '쿤', '쿠', '르', '스', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '[MASK]', '[MASK]', '▁중', '재', '▁등의', '▁활동', '도', '▁했다', '.', '[SEP]', '▁1979', '년', '닫', '▁1980', '년', '▁대한민국의', '▁정치적', '▁격', '변', '기', '▁당시의', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '▁보', '였고', ',', '▁이는', '[MASK]', '▁대한민국', '▁내에서', '▁고', '조', '되는', '▁반', '미', '[MASK]', '[MASK]', '▁한', '▁원', '인이', '▁', '됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박', '정', '희', '[MASK]', '[MASK]', '▁김', '재', '규', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁의해', '▁살해', '된', '▁것에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [9, 16, 18, 19, 55, 56, 60, 61, 67, 90, 98, 99, 114, 115, 119, 120, 121, 122], 'mask_label': ['▁제', '▁활동', '▁기', '니', '▁분', '쟁', '▁활동', '도', '▁~', '▁후에', '▁운동', '의', '▁대통령', '이', '▁중앙', '정보', '부', '장에']}\n",
      "enc_token: [5, 3612, 339, 1114, 238, 158, 3756, 3607, 231, 6, 49, 3692, 1654, 822, 209, 3623, 6, 228, 6, 6, 813, 3740, 3600, 1332, 311, 3635, 4956, 3937, 3699, 3626, 761, 3886, 95, 3729, 3624, 231, 947, 4437, 3598, 3599, 679, 1412, 4234, 4083, 770, 375, 3604, 1424, 3601, 313, 4333, 3614, 887, 3604, 605, 6, 6, 35, 3729, 507, 375, 3627, 345, 3599, 4, 2995, 3625, 4577, 1640, 3625, 447, 2843, 1032, 3889, 3614, 3195, 663, 1277, 202, 695, 433, 442, 3823, 3612, 227, 701, 47, 2470, 3604, 594, 6, 410, 3428, 70, 3676, 267, 141, 3686, 6, 6, 34, 129, 828, 3596, 1027, 3599, 131, 3662, 981, 3629, 3604, 338, 3642, 4055, 6, 6, 200, 3729, 3958, 6, 6, 6, 6, 355, 2591, 3711, 2057, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0   30    0    0    0    0\n",
      "    0    0  375    0   24 3733    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0  147\n",
      " 3972    0    0    0  375 3627    0    0    0    0    0  203    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0 1140    0    0    0    0    0    0    0\n",
      "  887 3601    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  663 3597    0    0    0  782 2275 3638 1312    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '상을', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '[MASK]', '[MASK]', '▁단', '체를', '▁직접', '▁만나', '▁분', '쟁', '의', '▁원', '인을', '▁근', '본', '적으로', '▁해결', '하기', '[MASK]', '▁힘', '썼', '다', '.', '▁이', '▁과정에서', '[MASK]', '▁행정', '부와', '▁갈', '등', '을', '▁보', '이기도', '▁했지만', ',', '▁전', '직', '▁대통령', '의', '▁권', '한', '과', '▁재', '야', '▁유명', '▁인사', '들의', '▁활약', '으로', '▁해결', '해', '▁나', '갔다', '.', '[SEP]', '▁카', '터', '는', '▁카', '터', '▁행정', '부', '▁이후', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '[MASK]', '[MASK]', '[MASK]', '▁전쟁', ',', '▁이', '라크', 'ر', '짝', '▁같이', '▁미국', '이', '▁블', '敵', '▁행', '동을', '▁최', '후', '로', '▁선택', '하는', '[MASK]', '[MASK]', '▁사고', '를', '▁버', '리고', '▁군사', '적', '▁행', '동을', '▁선', '행', '하는', '라고도', '▁간의', '[MASK]', '▁깊', '은', '▁유', '감을', '▁표시', '[MASK]', '[MASK]', '▁군사', '적', '▁활동', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 13, 28, 35, 80, 81, 82, 87, 88, 92, 93, 101, 102, 114, 115, 116, 122, 123], 'mask_label': ['▁인물', '▁및', '▁위해', '▁미국', '▁코', '소', '보', '▁전쟁', '과', '▁군사', '적', '▁전통', '적', '▁행', '위에', '▁대해', '▁하며', '▁미국의']}\n",
      "enc_token: [5, 460, 2324, 421, 15, 3800, 3601, 45, 333, 192, 3808, 3612, 6, 6, 164, 1396, 1069, 2142, 147, 3972, 3601, 129, 1171, 387, 3759, 127, 2317, 167, 6, 947, 4437, 3598, 3599, 8, 2208, 6, 895, 2576, 742, 3709, 3607, 47, 1304, 3379, 3604, 25, 3802, 663, 3601, 476, 3612, 3644, 174, 3775, 939, 3329, 247, 1102, 9, 2317, 3645, 58, 1133, 3599, 4, 207, 3714, 3602, 207, 3714, 895, 3638, 165, 243, 3597, 251, 4166, 45, 3614, 3604, 6, 6, 6, 506, 3604, 8, 3553, 5936, 4547, 733, 243, 3597, 754, 6599, 236, 1629, 130, 3706, 3603, 1715, 38, 6, 6, 1646, 3624, 407, 999, 1250, 3657, 236, 1629, 57, 3752, 38, 2815, 2714, 6, 1910, 3613, 46, 2196, 2466, 6, 6, 1250, 3657, 375, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0 1178  228\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  231    0    0    0    0    0    0  243    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  258 3688 3672    0\n",
      "    0    0    0  506 3644    0    0    0 1250 3657    0    0    0    0\n",
      "    0    0    0 1306 3657    0    0    0    0    0    0    0    0    0\n",
      "    0    0  236 1157  433    0    0    0    0    0 1368  679    0    0\n",
      "    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accepting-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "pediatric-bradley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29a21c015cc4859bd33c5f8ad3ec478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "manual-italic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   18, 3686,  207, 3714,    4,    6, 1042,  103, 3610, 3686,\n",
       "         3718,  207, 3714,   37, 3418,  416,  810, 3666, 3625,    6,    6,\n",
       "            7, 3629,    6,  241, 3602, 1114, 3724,  788,  243,   49, 3632,\n",
       "          796,  663, 1647, 3682, 3682, 3625,  203,    6,    6,    6,    6,\n",
       "            6,   18, 3686,  207, 3714, 3602, 1755, 3630, 3646,  630, 3714,\n",
       "         3565, 3835,  429, 3740, 3628, 3626, 1369,   10, 1605, 3599,    6,\n",
       "            6,    6,    6,    6,    6, 1135,   52, 3599,   13,   81,   87,\n",
       "         1501, 2247,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1232,   33,   52, 3599,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1509,  168, 3877,  414,  165, 1697, 4290, 3873,\n",
       "         3703, 3683,    6,   21, 5007,  399, 1927, 3607,  813,   17, 3599,\n",
       "          307,    6,    6,  103, 4313, 4290,    4], dtype=int32),\n",
       " memmap([   5,   41, 2429, 4143,  405, 1184, 3600, 2972,  173,    6,    6,\n",
       "          269,  820,  477, 3921,   66, 1357, 3715, 3633,  115, 3600, 2972,\n",
       "          173, 1345, 3604,   13,  574,   10,   15, 3784, 1117, 3238, 3617,\n",
       "         3706, 3596, 4639, 1364, 3627, 3648, 3643,  991, 2227,  262, 3651,\n",
       "         3616, 3607, 2142, 1148,  920,  111, 3314,    6, 3596, 4153,  658,\n",
       "            6,    6,    6,    6,    6,   15, 3784,   68, 3238, 3602,   13,\n",
       "            6, 1425,  173,  305, 3620, 1395, 5389, 2115,    6,  805, 3596,\n",
       "         4904, 3750, 3603, 4065,  115, 3617, 3756, 3596, 4639, 1364, 3627,\n",
       "          991, 3616, 3600,    7, 3614, 3746,    9, 2972,  173, 1345, 3604,\n",
       "          848, 3784, 3833,    8, 3637, 2263,   12, 3614, 3746,  836, 3596,\n",
       "         4904, 3750, 3603, 4065,  115, 3600, 2972,  173,  351, 3599,    4,\n",
       "          848, 3784, 3833,    8, 3637, 3676,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([   0,    0,    0,    0,    0,    0, 3324,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,  131, 3662,\n",
       "            0,    0,  203,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0, 3008, 3625, 3616,   16,\n",
       "         3599,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1755,\n",
       "         3630,   41, 3644,  830, 3624,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  593,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,  587,  931,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,  351, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0, 3185, 3596, 4153,  658,\n",
       "          171, 3599,   13,   81, 3604,    0,    0,    0,    0,    0,    0,\n",
       "          316,    0,    0,    0,    0,    0,  149, 3607,   19,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          848, 3784, 3833,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-traffic",
   "metadata": {},
   "source": [
    "## forming Bert Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "judicial-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "convertible-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "needed-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "later-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "immune-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "different-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "chicken-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "embedded-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "technological-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "specified-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "apparent-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "trained-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bizarre-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "unique-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "sticky-editor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 6s 6s/step - loss: 9.7511 - nsp_loss: 0.6926 - mlm_loss: 9.0585 - nsp_acc: 0.5000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.9492 - nsp_loss: 0.5777 - mlm_loss: 8.3715 - nsp_acc: 1.0000 - mlm_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fee7cc85790>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-bearing",
   "metadata": {},
   "source": [
    "## pretraining\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "deluxe-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "frequent-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "expected-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "another-handling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 128,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 512,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 128, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 512, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "nervous-fetish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 128), (None, 1850752     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            16768       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 1,867,520\n",
      "Trainable params: 1,867,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "talented-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 10000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "coordinate-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 338s 334ms/step - loss: 22.2993 - nsp_loss: 0.6773 - mlm_loss: 21.6220 - nsp_acc: 0.5462 - mlm_lm_acc: 0.0687\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.05616, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 336s 336ms/step - loss: 18.8989 - nsp_loss: 0.6369 - mlm_loss: 18.2620 - nsp_acc: 0.6062 - mlm_lm_acc: 0.0910\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.05616 to 0.10093, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 17.8664 - nsp_loss: 0.6236 - mlm_loss: 17.2427 - nsp_acc: 0.6273 - mlm_lm_acc: 0.1196\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.10093 to 0.12187, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 350s 350ms/step - loss: 17.3578 - nsp_loss: 0.6157 - mlm_loss: 16.7421 - nsp_acc: 0.6363 - mlm_lm_acc: 0.1279\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.12187 to 0.12909, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 335s 335ms/step - loss: 17.0721 - nsp_loss: 0.6091 - mlm_loss: 16.4630 - nsp_acc: 0.6448 - mlm_lm_acc: 0.1324\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.12909 to 0.13251, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 335s 335ms/step - loss: 16.8663 - nsp_loss: 0.6059 - mlm_loss: 16.2603 - nsp_acc: 0.6540 - mlm_lm_acc: 0.1344\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.13251 to 0.13508, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 16.7146 - nsp_loss: 0.6019 - mlm_loss: 16.1127 - nsp_acc: 0.6614 - mlm_lm_acc: 0.1369\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.13508 to 0.13690, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 16.5815 - nsp_loss: 0.5991 - mlm_loss: 15.9824 - nsp_acc: 0.6647 - mlm_lm_acc: 0.1379\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.13690 to 0.13806, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 16.5236 - nsp_loss: 0.5966 - mlm_loss: 15.9270 - nsp_acc: 0.6690 - mlm_lm_acc: 0.1387\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.13806 to 0.13867, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 16.5234 - nsp_loss: 0.5962 - mlm_loss: 15.9272 - nsp_acc: 0.6717 - mlm_lm_acc: 0.1392\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.13867 to 0.13900, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train_8000.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir_path}/bert_pre_train_8000.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-welcome",
   "metadata": {},
   "source": [
    "## evaluating result\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "coastal-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-behalf",
   "metadata": {},
   "source": [
    "## 회고\n",
    "---\n",
    "저번 노드보다 과제를 진행하는데 있어서 더 많은 코드가 노드에서 제공되어 과제를 진행하는데 조금 더\\\n",
    "수얼하게 진행되었습니다.\\\n",
    "다만, 해커톤 일정 전에 노드를 미리 끝내놓지 못 해서 코드와 과제에 관한 공부시간이 부족한 점은\\\n",
    "매우 아쉽습니다.\n",
    "\n",
    "BERT는 지속적으로 사용하게 될 모델 일 듯 하여, 반드시 공부를 하게 될 것입니다.\\\n",
    "(해커톤3의 주 모델 자체가 버트입니다.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
