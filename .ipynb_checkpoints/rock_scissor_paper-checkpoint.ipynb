{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suburban-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "# matplotlib.use('WebAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-switch",
   "metadata": {},
   "source": [
    "### 사용한 모듈들\n",
    "---\n",
    "중간의 matplotlib.use('WebAgg')는 프로젝트를 집컴퓨터의 vscode로 시도했기 때문입니다(이미지를 보기 위해 필요했습니다).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aquatic-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(img_path):\n",
    "    images = glob.glob(img_path + \"/*.jpg\")\n",
    "\n",
    "    print(len(images), \" images to be resized.\")\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size = (28, 28)\n",
    "    for img in images:\n",
    "        old_img = Image.open(img)\n",
    "        new_img = old_img.resize(target_size, Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "\n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "\n",
    "def load_data(img_path, number_of_data=1800):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size = 28\n",
    "    color = 3\n",
    "    # 이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs = np.zeros(number_of_data * img_size * img_size * color,\n",
    "                    dtype=np.int32).reshape(number_of_data, img_size, img_size, color)\n",
    "    labels = np.zeros(number_of_data, dtype=np.int32)\n",
    "\n",
    "    idx = 0\n",
    "    for file in glob.iglob(img_path + '/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype=np.int32)\n",
    "        imgs[idx, :, :, :] = img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx] = 0   # 가위 : 0\n",
    "        idx = idx + 1\n",
    "\n",
    "    for file in glob.iglob(img_path + '/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype=np.int32)\n",
    "        imgs[idx, :, :, :] = img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx] = 1   # 바위 : 1\n",
    "        idx = idx + 1\n",
    "\n",
    "    for file in glob.iglob(img_path + '/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file), dtype=np.int32)\n",
    "        imgs[idx, :, :, :] = img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx] = 2   # 보 : 2\n",
    "        idx = idx + 1\n",
    "\n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx, \"입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-people",
   "metadata": {},
   "source": [
    "### 노드에서 제공해준 함수 입니다.\n",
    "---\n",
    "거의 변형 없이 그대로 썼습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enormous-daughter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 5586 입니다.\n",
      "학습데이터(x_train)의 이미지 개수는 581 입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# image_dir_path = \"./data/paper\"\n",
    "# resize_images(image_dir_path)\n",
    "# image_dir_path = \"./data/rock\"\n",
    "# resize_images(image_dir_path)\n",
    "# image_dir_path = \"./data/scissor\"\n",
    "# resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/\"\n",
    "test_image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "\n",
    "# image_dir_path = \"./data\"\n",
    "# test_image_dir_path = \"./data/test\"\n",
    "\n",
    "img_number = 5586\n",
    "test_img_number = 581\n",
    "\n",
    "(x_train, y_train) = load_data(image_dir_path, img_number)\n",
    "(x_test, y_test) = load_data(test_image_dir_path, test_img_number)\n",
    "x_train_norm, x_test_norm = x_train / \\\n",
    "    255.0, x_test / 255.0   # 입력은 0~1 사이의 값으로 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-scout",
   "metadata": {},
   "source": [
    "### 이미지가 로딩되는 부분 입니다.\n",
    "---\n",
    "첫 번째 주석부분은 이미지를 리사이즈 할 때 사용했습니다. 당시 test폴더를 구분하지 않아서 한 번에 할 수 있었습니다.\n",
    "\n",
    "두 번째 주석부분은 집에서 읽을시 사용했던 경로입니다.\n",
    "\n",
    "받은 이미지를 모두 정리한 결과(못 받는 파일이 있어서 일부 제외했습니다. 구글 퍼미션), 6167개의 사진을 확보했고, 이 중 학습용 이미지를 일정하게 만들고 싶어서 각각 1862개 씩 배정했습니다. 테스트는 각각 보-190, 바위-194, 가위 190개로 배치했습니다.\n",
    "\n",
    "데이터를 나누는 과정에서 많은 시간이 소요됬고, 새로 배우는 linux/unix 명령어가 있었습니다.\n",
    "\n",
    "rename 's/^(\\d+)/100+$1/e' *.jpg\n",
    "\n",
    "숫자 이름에 100씩 더하기, 수를 변경해서 덮어쓰기를 막았습니다.\n",
    "\n",
    "rename -v 's/$/nr/' *\n",
    "\n",
    "글자 뒤에 첨가하거나 ^를 써서 앞에 첨가했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "permanent-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 122,051\n",
      "Trainable params: 122,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_channel_1 = 32\n",
    "n_channel_2 = 64\n",
    "n_dense = 64\n",
    "n_train_epoch = 3\n",
    "n_epoch = 25\n",
    "\n",
    "# print('최소값:', np.min(x_train_norm), ' 최대값:', np.max(x_train_norm))\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(\n",
    "    n_channel_1, (3, 3), activation='relu', input_shape=(28, 28, 3)))\n",
    "model.add(keras.layers.MaxPool2D(2, 2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(n_train_epoch, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-tobago",
   "metadata": {},
   "source": [
    "### 모델을 설정하는 부분입니다.\n",
    "---\n",
    "가위바위보가 숫자보다 조금 더 많은 특징을 파악해야한다고 생각해서 전반적인 값을 2배정도 상향했습니다.  \n",
    "그리고 가위, 바위 보 이므로 3개의 카테고리이고,\n",
    "마지막 n_epoch은 학습 횟수인데, 20번까지 정확도가 가감되면서 증가하다가,  \n",
    "23정도에서 1.000에 이르렀으므로, 25에서 멈추기로 했습니다.  \n",
    "\n",
    "input_shape의 마지막 매개변수를 3으로 변경했습니다. 그림이 흑백이 아니라서 3이라고 생각합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "military-sustainability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "175/175 [==============================] - 7s 18ms/step - loss: 1.0551 - accuracy: 0.4260\n",
      "Epoch 2/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.7414\n",
      "Epoch 3/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.3347 - accuracy: 0.8821\n",
      "Epoch 4/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.2206 - accuracy: 0.9291\n",
      "Epoch 5/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.1436 - accuracy: 0.9575\n",
      "Epoch 6/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0886 - accuracy: 0.9774\n",
      "Epoch 7/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0810 - accuracy: 0.9772\n",
      "Epoch 8/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0429 - accuracy: 0.9919\n",
      "Epoch 9/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0377 - accuracy: 0.9922\n",
      "Epoch 10/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0220 - accuracy: 0.9951\n",
      "Epoch 11/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0140 - accuracy: 0.9982\n",
      "Epoch 12/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.1298 - accuracy: 0.9602\n",
      "Epoch 13/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0115 - accuracy: 0.9987\n",
      "Epoch 14/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0094 - accuracy: 0.9993\n",
      "Epoch 15/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 0.0061 - accuracy: 0.9995\n",
      "Epoch 16/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9997\n",
      "Epoch 17/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9995\n",
      "Epoch 18/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0153 - accuracy: 0.9952\n",
      "Epoch 19/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0519 - accuracy: 0.9850\n",
      "Epoch 20/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0096 - accuracy: 0.9976\n",
      "Epoch 21/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "175/175 [==============================] - 1s 4ms/step - loss: 9.9438e-04 - accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "175/175 [==============================] - 1s 3ms/step - loss: 7.5862e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3138ada090>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-reducing",
   "metadata": {},
   "source": [
    "### 모델을 생성하는 부분입니다.\n",
    "---\n",
    "노드에서는 x_train_norm의 모양을 변경해줘야 했지만, 현 프로젝트에서는 그럴 필요가 없었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "touched-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 - 2s - loss: 0.0300 - accuracy: 0.9845\n",
      "test_loss: 0.030021443963050842 \n",
      "test_accuracy: 0.9845094680786133\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-webmaster",
   "metadata": {},
   "source": [
    "### 실제 테스트 부분입니다.\n",
    "---\n",
    "\n",
    "학습시의 1.0의 정확도를 보였지만 테스트에서는 1.0의 정확도를 이루지는 못 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controversial-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측확률분포: [1.3894066e-01 3.3731008e-05 8.6102557e-01]\n",
      "라벨: 0, 예측결과: 2\n",
      "To view figure, visit http://127.0.0.1:8988\n",
      "예측확률분포: [9.7298878e-01 1.6710766e-09 2.7011245e-02]\n",
      "라벨: 2, 예측결과: 0\n",
      "To view figure, visit http://127.0.0.1:8988\n",
      "예측확률분포: [8.2305622e-01 1.7694248e-01 1.3111560e-06]\n",
      "라벨: 1, 예측결과: 0\n",
      "To view figure, visit http://127.0.0.1:8988\n",
      "예측확률분포: [0.4974044  0.46261185 0.03998368]\n",
      "라벨: 1, 예측결과: 0\n",
      "To view figure, visit http://127.0.0.1:8988\n",
      "예측확률분포: [9.7298878e-01 1.6710766e-09 2.7011245e-02]\n",
      "라벨: 2, 예측결과: 0\n",
      "To view figure, visit http://127.0.0.1:8988\n"
     ]
    }
   ],
   "source": [
    "# 오류확인용\n",
    "predicted_result = model.predict(x_test_norm)  # model이 추론한 확률값.\n",
    "predicted_labels = np.argmax(predicted_result, axis=1)\n",
    "\n",
    "import random\n",
    "wrong_predict_list = []\n",
    "for i, _ in enumerate(predicted_labels):\n",
    "    # i번째 test_labels과 y_test이 다른 경우만 모아 봅시다.\n",
    "    if predicted_labels[i] != y_test[i]:\n",
    "        wrong_predict_list.append(i)\n",
    "\n",
    "# wrong_predict_list 에서 랜덤하게 5개만 뽑아봅시다.\n",
    "samples = random.choices(population=wrong_predict_list, k=5)\n",
    "\n",
    "for n in samples:\n",
    "    print(\"예측확률분포: \" + str(predicted_result[n]))\n",
    "    print(\"라벨: \" + str(y_test[n]) + \", 예측결과: \" + str(predicted_labels[n]))\n",
    "    plt.imshow(x_test[n], cmap=plt.cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-emission",
   "metadata": {},
   "source": [
    "   ### 잘 못 검증된 사진 확인 부분입니다\n",
    "   ---\n",
    "   육안으로 확인하기 위해 시도했습니다.  \n",
    "   노드의 코드에서 변경된 것은 없습니다.  \n",
    "   육안으로 식별이 잘 안 되는 사진이 많지만, 크기를 줄여서 그런 것 같습니다.  \n",
    "   컴퓨터도 해상도 때문에 이미지크기에 정확도가 영향 받을 수 있다는 생각을 했습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-convergence",
   "metadata": {},
   "source": [
    "## 마치며\n",
    "---\n",
    "처음으로 인공지능 프로그램을 만들어 볼 수 있어서 좋았습니다.  \n",
    "인공지능 프로그램이 기본적으로 갖추는 구조를 배울 수 있었던 것 같습니다.  \n",
    "모델에 대해서 이해를 잘 하지 못 한 것 같지만, 추후 보충해 나갈 수 있을 것이라고 기대합니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-socket",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
